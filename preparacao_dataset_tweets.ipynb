{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = pd.read_csv('/home/filipe/Documentos/Redes Complexas/Codigos/7-ProjetoFinal/data/training.1600000.processed.noemoticon.csv',header=None, names=nomes,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target')['target'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando um grafo direcionado\n",
    "G = nx.Graph()\n",
    "\n",
    "# Coletando todos os usuários do conjunto\n",
    "df['user'].apply(lambda x: G.add_node(x))\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for index, line in df.iterrows():\n",
    "    replieds = re.findall(r\"@(\\w+)\", df.iloc[index,5])\n",
    "    if len(replieds) > 2: \n",
    "        for replied in replieds: \n",
    "            if G.has_node(replied) and G.has_node(df.iloc[index,4]):\n",
    "                G.add_edge(df.iloc[index,4], replied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "connected = nx.connected_components(G)  # Para grafos direcionados\n",
    "\n",
    "# Selecionando o maior subgrafo\n",
    "maior = max(connected, key=len)\n",
    "G = G.subgraph(maior)\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def calculate_measures(G):\n",
    "    \n",
    "    def degree_distribution(G):\n",
    "        vk = dict(G.degree())\n",
    "        vk = list(vk.values())\n",
    "        vk = np.array(vk)\n",
    "        maxk = np.max(vk)\n",
    "        mink = np.min(vk)\n",
    "        kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "        Pk = np.zeros(maxk+1) # P(k)\n",
    "        for k in vk:\n",
    "            Pk[k] = Pk[k] + 1\n",
    "        Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "        return kvalues,Pk\n",
    "    \n",
    "    def momment_of_degree_distribution(G,m):\n",
    "        M = 0\n",
    "        N = len(G)\n",
    "        for i in G.nodes:\n",
    "            M = M + G.degree(i)**m\n",
    "        M = M/N\n",
    "        return M\n",
    "\n",
    "    def shannon_entropy(G):\n",
    "        k,Pk = degree_distribution(G)\n",
    "        H = 0\n",
    "        for p in Pk:\n",
    "            if(p > 0):\n",
    "                H = H - p*math.log(p, 2)\n",
    "        return H\n",
    "    \n",
    "    # Número de Vértices\n",
    "    nN = G.number_of_nodes()                    \n",
    "    print('Number of nodes', nN)\n",
    "    \n",
    "    # Número de Arestas\n",
    "    nE = G.number_of_edges()                    \n",
    "    print('Number of edges', nE)\n",
    "    \n",
    "    # Primeiro momento\n",
    "    k1 = momment_of_degree_distribution(G,1)    \n",
    "    print('1º Momentum', k1)\n",
    "    \n",
    "    # Segundo momento\n",
    "    k2 = momment_of_degree_distribution(G,2)    \n",
    "    print('2º Momentum', k2)\n",
    "    \n",
    "    # Variância\n",
    "    variance = k2 - k1**2                       \n",
    "    print('Variância Momentum', variance)\n",
    "    \n",
    "    # Average Clustering\n",
    "    av_cl = nx.average_clustering(G)\n",
    "    print('Average Clustering', av_cl)\n",
    "    \n",
    "    # Entropia de Shannon\n",
    "    sh_ent = shannon_entropy(G)\n",
    "    print('Shannon Entropy', sh_ent)\n",
    "    \n",
    "    # Transitivitidade: Quantidade de Triângulos de um Grafo\n",
    "    trans = nx.transitivity(G)                      \n",
    "    print('Transitivity', trans)\n",
    "\n",
    "    # Diâmetro\n",
    "    diameter = nx.diameter(G)                   \n",
    "    print('Diameter', diameter)\n",
    "\n",
    "    # Eficiência Global da Informação (Eficiência do Grafo)\n",
    "    gl_ef = nx.global_efficiency(G)             \n",
    "    print('Global Efficiency', gl_ef)\n",
    "    \n",
    "    # Eficiência Local da Informação \n",
    "    lc_ef = nx.local_efficiency(G)              \n",
    "    print('Local Efficiency', lc_ef)\n",
    "    \n",
    "    # Average Shortest Path Lenght\n",
    "    l = nx.average_shortest_path_length(G)      \n",
    "    print('Average Shortest Path Lenght', l)\n",
    "    \n",
    "    # Grau de Assortatividade\n",
    "    r = nx.degree_assortativity_coefficient(G)  \n",
    "    print('Assortativity Coefficient', r)\n",
    "\n",
    "\n",
    "    ################## Medidas de centralidade ####################################\n",
    "    # Grau Médio\n",
    "    g = np.mean(list(dict(G.degree()).values()))\n",
    "    print('Mean Degree', g)\n",
    "    \n",
    "    # Média Closeness Centrality\n",
    "    cl_cent = np.mean(list(nx.closeness_centrality(G).values())) \n",
    "    print('Closeness Centrality', cl_cent)\n",
    "    \n",
    "    # Média Betweenness Centrality\n",
    "    bet_cent = np.mean(list(dict(nx.betweenness_centrality(G)).values())) \n",
    "    print('Betwenees Centrality', bet_cent)\n",
    "\n",
    "    # Average Eigenvector Centrality\n",
    "    eig_value = np.mean(list(dict(nx.eigenvector_centrality(G, max_iter = 1000)).values()))\n",
    "    print('Average Eigenvector Centrality', eig_value)\n",
    "\n",
    "    # Page Rank\n",
    "    page_rank = np.mean(list(dict(nx.pagerank(G, alpha=0.85, max_iter=1000, weight='weight')).values()))\n",
    "    print('Page rank', page_rank)\n",
    "\n",
    "    # K-Core\n",
    "    #kcore = np.mean(list(dict(nx.core_number(G)).values()))\n",
    "    #print('KCore', kcore)\n",
    "    kcore=0\n",
    "    return nN, nE, k1, k2, variance, av_cl, sh_ent,trans, diameter, gl_ef, lc_ef,l, g, cl_cent, bet_cent, eig_value, kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_measures(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, \"grafo_tweets.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=50, font_size=8, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_keywords(G, number = 10):\n",
    "    \n",
    "    degree_scores = dict(G.degree())\n",
    "    sorted_degree = sorted(degree_scores, key=degree_scores.get, reverse=True)[:number]\n",
    "    print(sorted_degree)\n",
    "\n",
    "    pagerank_scores = nx.pagerank(G)\n",
    "    sorted_pagerank = sorted(pagerank_scores, key=pagerank_scores.get, reverse=True)[:number]\n",
    "    print(sorted_pagerank)\n",
    "    \n",
    "    betweenness_scores = nx.betweenness_centrality(G)\n",
    "    sorted_betweenness = sorted(betweenness_scores, key=betweenness_scores.get, reverse=True)[:number]\n",
    "    print(sorted_betweenness)\n",
    "    \n",
    "    closeness_scores = nx.closeness_centrality(G)\n",
    "    sorted_closeness = sorted(closeness_scores, key=closeness_scores.get, reverse=True)[:number]\n",
    "    print(sorted_closeness)\n",
    "    \n",
    "    eigenvector_scores = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    sorted_eigenvector = sorted(eigenvector_scores, key=eigenvector_scores.get, reverse=True)[:number]\n",
    "    print(sorted_eigenvector)\n",
    "\n",
    "    return sorted_degree, sorted_pagerank, sorted_betweenness, sorted_closeness, sorted_eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_keywords(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "df_filtered = df.loc[df['user'].isin(nodes)].reset_index()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):  \n",
    "\n",
    "    # Retira toda acentuação, cedilhas e normaliza em minúsculas\n",
    "    text = normalize('NFKD', text).encode('ASCII','ignore').decode('ASCII').lower()\n",
    "\n",
    "    # Retira todos os dígitos\n",
    "    text = re.sub('\\d+', \"\", text)\n",
    "    \n",
    "    # Retira toda pontuação com variações acrescentadas\n",
    "    to_get_off = ['¹','²', '³', 'º', 'ª', '==', ',', ':', '\\\"', '\\'']\n",
    "    to_get_off.extend([*punctuation])\n",
    "    text = \"\".join([char if char not in to_get_off else ' ' for char in text])\n",
    "    \n",
    "    # Remove espaços sobrando\n",
    "    text = re.sub('\\s+', \" \", text).split(\" \")\n",
    "            \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph(palavras):\n",
    "\n",
    "    # Criar grafo\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Adicionar nós ao grafo na ordem em que aparecem na lista\n",
    "    for i in range(len(palavras) - 1):\n",
    "        if not G.has_node(palavras[i]):\n",
    "            G.add_node(palavras[i])\n",
    "        if not G.has_node(palavras[i + 1]):\n",
    "            G.add_node(palavras[i + 1])\n",
    "    \n",
    "    # Adicionar arestas ao grafo\n",
    "    for i in range(len(palavras) - 1):\n",
    "        G.add_edge(palavras[i], palavras[i + 1])\n",
    "\n",
    "    # Tranformação para Grafo Não-Direcionado\n",
    "    G = G.to_undirected()\n",
    "\n",
    "    # Removendo Self Loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Ordenando componentes por ordem de tamanho, selecionando o maior\n",
    "    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    G = G.subgraph(Gcc[0])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "df_filtered['preprocessed'] = df_filtered['text'].apply(lambda x: list(word.text for word in nlp(preprocessing_text(str(x))) if not word.is_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.DataFrame(columns=[\n",
    "    'Keyword Degree','Keyword Pagerank','Keyword Betweenness','Keyword Closeness',\n",
    "    'Keyword Eigenvector'])\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=[\n",
    "    'Nodes','Edges','Momento 1','Momento 2','Variance','Average Clustering',\n",
    "    'Shannon Entropy','Transitivity','Diameter', 'Global Efficiency', 'Local Efficiency',\n",
    "    'Average Shortest Path', 'Mean Degree', 'Average Closeness Centrality', \n",
    "    'Average Betweenness Centrality', 'Average Eigenvector Centrality', 'K-Core'])\n",
    "\n",
    "# Controle\n",
    "x=0\n",
    "for words in df_filtered['preprocessed']:\n",
    "    \n",
    "    # Controle para acompanhar progresso do processamento\n",
    "    x+=1\n",
    "    print(x)\n",
    "    # Preparação do Grafo de Coocorrência - janela de 2 palavras \n",
    "    # (é realizado preprocessamento do grafo)\n",
    "    \n",
    "    try:\n",
    "        G = prepare_graph(words)\n",
    "\n",
    "\n",
    "        # Calculando Métricas Quantitativas\n",
    "        measures = calculate_measures(G)\n",
    "        df_metrics.loc[df_metrics.shape[0]] = {\n",
    "            'Nodes': measures[0],\n",
    "            'Edges': measures[1],\n",
    "            'Momento 1': measures[2],\n",
    "            'Momento 2': measures[3],\n",
    "            'Variance': measures[4],\n",
    "            'Average Clustering': measures[5],\n",
    "            'Shannon Entropy': measures[6],\n",
    "            'Transitivity': measures[7],\n",
    "            'Diameter': measures[8],\n",
    "            'Global Efficiency': measures[9],\n",
    "            'Local Efficiency': measures[10],\n",
    "            'Average Shortest Path': measures[11],\n",
    "            'Mean Degree': measures[12],\n",
    "            'Average Closeness Centrality': measures[13],\n",
    "            'Average Betweenness Centrality': measures[14],\n",
    "            'Average Eigenvector Centrality': measures[15],\n",
    "            'K-Core': measures[16]\n",
    "        }\n",
    "\n",
    "        # Coletando Keywords (com medidas de centralidade)\n",
    "        sorted_keywords = identify_keywords(G, 10)\n",
    "        keywords.loc[keywords.shape[0]] = {\n",
    "            'Keyword Degree':sorted_keywords[0],\n",
    "            'Keyword Pagerank':sorted_keywords[1],\n",
    "            'Keyword Betweenness':sorted_keywords[2],\n",
    "            'Keyword Closeness':sorted_keywords[3],\n",
    "            'Keyword Eigenvector':sorted_keywords[4]\n",
    "        }\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data = pd.concat([df_filtered, df_metrics, keywords], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data['target'] = consolidated_data['target'].apply(lambda x: 'positive' if x > 0 else 'negative')\n",
    "consolidated_data.to_csv('dataset_tweets_consolidated.csv', index=False)\n",
    "consolidated_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
